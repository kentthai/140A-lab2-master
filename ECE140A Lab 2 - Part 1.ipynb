{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE140A Lab 2 - Part 1: Collision Avoidance\n",
    "Due: 27 January 2020 (Monday Lab session)  \n",
    "29 January 2020 (Wednesday Lab session)  \n",
    "31 January 2020 (Friday Lab session)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collision Avoidance\n",
    "\n",
    "In lab 1, you assembled the JetBot and moved it around in a square. Hopefully, now you understand how make the robot move around by interacting with the ``jetbot`` library.\n",
    "\n",
    "What would be more interesting, however, is to move the JetBot around autonomously! While complete autonomy is a very difficult task, we can break it down into easier subproblems. Arguably, one of the most important subproblems to solve is to prevent the robot from entering dangerous situations: bumping into things or falling off cliffs! Let's call this *collision avoidance*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use deep learning with camera input to solve collision avoidance. We will do this in a simple way:\n",
    "\n",
    "1. Data collection: We'll manually place the robot in situations where its \"safety bubble\" is violated, and label the corresponding camera images as ``blocked``. And we'll also place the robot in scenarios where it's safe to move forward a bit, and label the corresponding images as ``free``.\n",
    "\n",
    "2. Training/Fine-tuning: Using the images collected in the previous step, we'll train a deep neural network to predict if it is ``blocked`` or ``free`` from the current image in the camera feed. Instead of training a network from scratch, we'll \"fine-tune\" or improve the model already trained by the folks at NVIDIA.\n",
    "\n",
    "3. Testing and improving failure cases: We'll run our JetBot with collision avoidance to see how it performs. Taking note of scenarios where the robot doesn't avoid collision as expected, we'll collect some more data from those scenarios.\n",
    "\n",
    "Repeat steps 2 and 3 as many times (not zero!) as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There's a lot of \"advanced\" code in this notebook. You don't need to understand it all. We'll just say, in words, what the code does, and you can assume that it works as we say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "Data collection sounds like the most tedious thing ever, and it can be. However, because we're using a pretrained neural network (more on that later), we don't need to collect a lot of data. We will just collect 50 images or more per class (100 or more images in total). (It will just take 10 minutes)\n",
    "\n",
    "Later, the TAs will combine the data collected by all the students to make a super collision avoider!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display live camera feed\n",
    "\n",
    "Let's initialize and display our camera. Our neural network takes a 224x224 pixel image as input. We'll set our camera to that size to minimize the filesize of our dataset.\n",
    "\n",
    "> It's not important to understand how to code below works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create a few directories where we'll store our data: a folder ``dataset`` that will contain two subfolders ``free`` and ``blocked``, where we'll place images for each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "blocked_dir = 'dataset/blocked'\n",
    "free_dir = 'dataset/free'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(free_dir)\n",
    "    os.makedirs(blocked_dir)\n",
    "except FileExistsError:\n",
    "    print('Directories not created becasue they already exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh the Jupyter file browser on the left to see those directories appear. Next, let's create some buttons that we'll use to save snapshots. Click ``free`` if the current image corresponds to the robot having some space to move forward. Click ``blocked`` if there is an obstacle in front of the robot such that it will not be able to move any further or rotate in place. It's better to be conservative here, that is, if you're not sure if a situation is ``free`` or ``blocked`` it's better to label it as ``blocked``.\n",
    "\n",
    "> Again, you don't have to understand the following code.\n",
    "\n",
    "> The counts displayed in the widgets can be off by 1 (or sometimes even more) because they count all the files in the blocked and free folders (not just images). Continue to the next part only if **our checker** says you're good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button_layout = widgets.Layout(width='128px', height='64px')\n",
    "free_button = widgets.Button(description='add free', button_style='success', layout=button_layout)\n",
    "blocked_button = widgets.Button(description='add blocked', button_style='danger', layout=button_layout)\n",
    "free_count = widgets.IntText(layout=button_layout, value=len(os.listdir(free_dir)))\n",
    "blocked_count = widgets.IntText(layout=button_layout, value=len(os.listdir(blocked_dir)))\n",
    "\n",
    "display(widgets.HBox([free_count, free_button]))\n",
    "display(widgets.HBox([blocked_count, blocked_button]))\n",
    "\n",
    "from uuid import uuid1\n",
    "\n",
    "def save_snapshot(directory):\n",
    "    image_path = os.path.join(directory, str(uuid1()) + '.jpg')\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(image.value)\n",
    "\n",
    "def save_free():\n",
    "    global free_dir, free_count\n",
    "    save_snapshot(free_dir)\n",
    "    free_count.value = len(os.listdir(free_dir))\n",
    "    \n",
    "def save_blocked():\n",
    "    global blocked_dir, blocked_count\n",
    "    save_snapshot(blocked_dir)\n",
    "    blocked_count.value = len(os.listdir(blocked_dir))\n",
    "    \n",
    "# attach the callbacks, we use a 'lambda' function to ignore the\n",
    "# parameter that the on_click event would provide to our function\n",
    "# because we don't need it.\n",
    "free_button.on_click(lambda x: save_free())\n",
    "blocked_button.on_click(lambda x: save_blocked())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checker import check_collected_images\n",
    "check_collected_images('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing an understanding of the Neural Network\n",
    "Below are some resources which introduce the concept of neural networks : \n",
    "> https://towardsdatascience.com/a-gentle-introduction-to-neural-networks-series-part-1-2b90b87795bc\n",
    "\n",
    "> https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is PyTorch?\n",
    "* An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
    "* For more reading please visit\n",
    ">http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "#### Getting Started\n",
    "* NOTE: Make sure that you have numpy and pytorch installed before importing the python frameworks (they are on the JetBot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors\n",
    "* Torch tensors are similar to NumPy ndarrays, with the addition being that they can also be used on a GPU to accelerate computing. \n",
    "* The documentation for tensors is available here \n",
    "> http://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Construct a 5 Ã— 3 tensor and print it with the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Construct a randomly initialized tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We can also directly initialize tensors with prescribed values. Create the following two tensors. What are their shapes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[-0.1859, 1.3970, 0.5236],\n",
    "[ 2.3854, 0.0707, 2.1970],\n",
    "[-0.3587, 1.2359, 1.8951],\n",
    "[-0.1189, -0.1376, 0.4647],\n",
    "[-1.8968, 2.0164, 0.1092]])\n",
    "y = torch.Tensor([[ 0.4838, 0.5822, 0.2755],\n",
    "[ 1.0982, 0.4932, -0.6680],\n",
    "[ 0.7915, 0.6580, -0.5819],\n",
    "[ 0.3825, -1.1822, 1.5217],\n",
    "[ 0.6042, -0.2280, 1.3210]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Adding two tensors is fairly straightforward. There are multiple syntaxes for this operation. Sum x and y (from question 4) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x.add(y))\n",
    "torch.add(x, y, out=x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. To reshape a tensor, you can use torch.view. Interpret the effect of each of these instructions. What does the -1 mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Go through the documentation and get familiar with other operations as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd: automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A central feature in PyTorch is the autograd package. It provides tools performing automatic differentiation for all operations based on Torch tensors. This means that the gradients of such operations will not require to be explicitly programmed. \n",
    "* It uses a define-by-run framework that computes these gradients dynamically during runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once you have developed a basic understanding of Neural Networks you can look into the following pyTorch tutorials\n",
    "\n",
    ">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "\n",
    ">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : The required output from the above exercise is that you are able to understand and run the code of the above two notebooks as it will be used later in the Lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Go Deeper: Training a Powerful Deep Neural Network\n",
    "\n",
    "Now you know a little bit about different building blocks (layers) of neural networks and can construct a network with PyTorch. The network you wrote is a toy model and can work well for relatively simple tasks. However, for more complex tasks like collision avoidance and path following, we'll need a more powerful neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the definition of `DeepDeepNet` using the given image.\n",
    "\n",
    "> Don't be intimidated by the size of the diagram; each block corresponds to just 1-2 lines of code! Pro-tip: download the image and look at it in another window as you write the code.\n",
    "\n",
    "![DeepDeepNet.png](DeepDeepNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepDeepNet, self).__init__()\n",
    "        self.conv1 = ...\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.mp1 = ...\n",
    "        self.conv2 = ...\n",
    "        self.relu2 = ...\n",
    "        self.mp2 = ...\n",
    "        self.conv3 = ...\n",
    "        self.relu3 = ...\n",
    "        self.conv4 = ...\n",
    "        self.relu4 = ...\n",
    "        self.conv5 = ...\n",
    "        self.relu5 = ...\n",
    "        self.mp3 = ...\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.linear1 = ...\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout2 = ...\n",
    "        self.linear2 = ...\n",
    "        self.relu7 = ...\n",
    "        self.linear3 = nn.Linear(4096, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        \n",
    "        # Complete code here such that x is the output just before the avgpool layer\n",
    "        # after \"passing through\" all the previous layers\n",
    "        \n",
    "        ### End of your code ###\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Complete code here such that x is the output after the linear3 layer after\n",
    "        # \"passing through\" all the previous layers\n",
    "        \n",
    "        ### End of your code ###\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the Deep Neural Network\n",
    "\n",
    "If we train such a deep network from scratch on just 50-100 images, it will not be able to learn anything meaningful, and if we had a lot more  data, it would take a very long time to train (especially if you don't have a good GPU). But we don't need to! There exist pretrained models which we can train just a little more (with our 100 images); this is called *fine-tuning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the pretrained weights to the model you've written (it might take ~20s the first time).\n",
    "\n",
    "> If this step is successfull then your written model is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepDeepNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checker import check_and_apply_weights\n",
    "nvidia_weights = torch.load('best_model_nvidia.pth')\n",
    "check_and_apply_weights(model, nvidia_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use our collected images to (slightly) improve the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the ``ImageFolder`` dataset class available with the ``torchvision.datasets`` package. This helps us work with datasets organized into folders.  We attach transforms from the ``torchvision.transforms`` package to prepare the data for training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    'dataset',\n",
    "    transforms.Compose([\n",
    "        transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the dataset into *training* and *test* sets. A training set is the data used to (surprise, surprise) *train* the model. The test set will be used to verify the accuracy of the model we train. We never use the test set for training/improving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data loaders to load data in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create two ``DataLoader`` instances, which provide utilities for shuffling data, producing *batches* of images, and loading the samples in parallel with multiple workers (processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the neural network (finally!)\n",
    "\n",
    "Using the code below we will train the neural network for 20 epochs, saving the best performing model after each epoch. \n",
    "\n",
    "> An epoch is one pass through all the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This will take 10-20 minutes. Go complete a chore or something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BEST_MODEL_PATH = 'best_model.pth'\n",
    "best_accuracy = 0.0\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    test_error_count = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_error_count += float(torch.sum(torch.abs(labels - outputs.argmax(1))))\n",
    "    \n",
    "    test_accuracy = 1.0 - float(test_error_count) / float(len(test_dataset))\n",
    "    print('Epoch %d: Accuracy = %f' % (epoch + 1, test_accuracy))\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Collision Avoidance\n",
    "\n",
    "Let's first load the best model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepDeepNet()\n",
    "model.model.load_state_dict(torch.load('best_model.pth'))\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the preprocessing function\n",
    "\n",
    "We have now loaded our model, but there's a slight issue.  The format that we trained our model doesnt *exactly* match the format of the camera.  To do that, we need to do some *preprocessing*.  \n",
    "\n",
    "> Again, don't worry if this doesn't make sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\n",
    "\n",
    "Now, let's start and display our camera.  You should be pretty familiar with this by now.  We'll also create a slider that will display the\n",
    "probability that the robot is blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = Camera.instance(width=224, height=224)\n",
    "print('Camera instance created.')\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "print('Image instance created.')\n",
    "blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.HBox([image, blocked_slider]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create our robot instance which we'll need to drive the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision avoidance logic\n",
    "\n",
    "Next, we'll create a function that will get called whenever the camera's value changes.  This function will do the following steps\n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network\n",
    "3. While the neural network output indicates we're blocked, we'll turn left, otherwise we go forward.\n",
    "\n",
    "In the code block below, `prob_block` is the output of the neural network which is a number in $[0, 1]$. Complete the code below such that the robot moves forward if `prob_block` < 0.5 with a speed of `0.20`. Otherwise, it moves its left wheel with a speed of `0.20` (spinning in place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def update(change):\n",
    "    global blocked_slider, robot\n",
    "    x = change['new'] \n",
    "    x = preprocess(x)\n",
    "    y = model(x)\n",
    "    \n",
    "    # we apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    \n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "\n",
    "    blocked_slider.value = prob_blocked\n",
    "\n",
    "    ### Write your code here ###\n",
    "    \n",
    "    ### End of your code ###\n",
    "\n",
    "    \n",
    "    time.sleep(0.001)\n",
    "        \n",
    "update({'new': camera.value})  # we call the function once to intialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing. \n",
    "\n",
    "We accomplish that with the ``observe`` function.\n",
    "\n",
    "> WARNING: This code will move the robot!! Please make sure your robot has clearance.  The collision avoidance should work, but the neural\n",
    "> network is only as good as the data it's trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame.  Perhaps start by placing your robot on the ground and seeing what it does when it reaches an obstacle.\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camera.unobserve(update, names='value')\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you want the robot to run without streaming video to the browser.  You can unlink the camera as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_link.unlink()  # don't stream to browser (will still run camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue streaming call the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_link.link()  # stream to browser (wont run camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's it for this live demo! Hopefully you had some fun and your robot avoided collisions intelligently!\n",
    "\n",
    "If your robot wasn't avoiding collisions very well, try to spot where it fails. The beauty is that we can collect more data for these failure scenarios and the robot should get even better :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
